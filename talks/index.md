---
layout: default
title: Talks
---

Our workshop in 2023 will feature keynote addresses from four exciting speakers: Madiha Tahir, Lama Ahmad, Hadi Salman, and Afra Feyza Akyürek. 

<table>
<tr>
	<td width="50%"><img src="{{site.baseurl}}/images/madiha_tahir.jpeg" width="100px" align="bottom"></td>
	<td width="50%"><img src="{{site.baseurl}}/images/lama_ahmad.jpeg" width="100px" align="bottom"></td>
</tr>
<tr>
	<td><b>Madiha Tahir</b><br /><a href="#madiha">Title TBD</a></td>
	<td><b>Lama Ahmad</b><br /><a href="#lama">Red Teaming Generative AI Systems</a></td>
</tr>
<tr>
	<td width="50%"><img src="{{site.baseurl}}/images/hadi_salman.jpeg" width="100px" align="bottom"></td>
	<td width="50%"><img src="{{site.baseurl}}/images/feyza_akyurek.jpeg" width="100px" align="bottom"></td>
</tr>
<tr>
	<td><b>Hadi Salman</b><br /><a href="#hadi">Adversarial Examples Beyond Security</a></td>
	<td><b>Afra Feyza Akyürek</b><br /><a href="#feyza">Editing Language Models with Natural Language Feedback</a></td>
</tr>
</table>

---

<section id="madiha">
<h2>Madiha Tahir</h2>
</section>

<img src="{{site.baseurl}}/images/madiha_tahir.jpeg" width="170px" align="bottom">

#### Talk title

TBD

#### Abstract

TBD

#### Speaker Biography

Madiha Tahir is an Assistant Professor of American Studies and the co-director of the Yale Ethnography Hub. She is an interdisciplinary scholar of technology and war with interest and expertise in digital war, surveillance, militarism, and empire and technology studies from below. Her work intersects the anthropology of war with insights from the fields of postcolonial, South Asian, and Black Studies literatures to reframe our understanding of technology, war, and US imperialism.

---

<section id="lama">
<h2>Lama Ahmad</h2>
</section>

<img src="{{site.baseurl}}/images/lama_ahmad.jpeg" width="170px" align="bottom">

#### Talk title

Red Teaming Generative AI Systems

#### Abstract

As generative AI systems continue to evolve, it is crucial to rigorously evaluate their robustness, safety, and potential for misuse. In this talk, we will explore the application of red teaming methodologies to assess the vulnerabilities and limitations of these cutting-edge technologies. By simulating adversarial attacks and examining system responses, we aim to uncover latent risks and propose effective countermeasures to ensure the responsible deployment of generative AI systems in new domains and modalities.

#### Speaker Biography

Lama Ahmad is a Policy Researcher at OpenAI on the Trustworthy AI Team, where she works on conducting analyses to prepare for safe and successful deployment of increasingly advanced AI. At OpenAI, Lama leads external red teaming efforts on models such as GPT-4, DALL-E 3, and other frontier AI systems as well as the Researcher Access Program. Prior to OpenAI, Lama was on the Open Research & Transparency Team at Meta, where she facilitated collaborative research on the impact of Facebook and Instagram on U.S. elections, and helped build products for privacy preserved data sharing for research. Lama was also a Luce Scholar at the UN Global Pulse Lab in Jakarta, Indonesia after graduating from NYU Abu Dhabi, where she studied Social Research and Public Policy and Interactive Media and Technology.

---

<section id="hadi">
<h2>Hadi Salman</h2>
</section>

<img src="{{site.baseurl}}/images/hadi_salman.jpeg" width="170px" align="bottom">

#### Talk Title

Adversarial examples beyond security

#### Abstract

Adversarial examples are often perceived as threats that deceive AI models, posing security risks. This talk aims to reframe adversarial examples as beneficial tools, emphasizing their positive impact on AI deployment. Specifically, we will discuss their application in two key areas: designing robust objects and safeguarding against unauthorized AI-based image manipulations. Our discussion will offer a nuanced perspective on the role of adversarial examples in AI.

#### Speaker Biography

Hadi is a Research Scientist at OpenAI. He recently got a PhD in Computer Science from MIT, where he was advised by Aleksander Madry. His work has focused on the development of robust and reliable machine learning systems to ensure their safe and responsible deployment in real-world scenarios. Before MIT, he spent a few years at Microsoft Research also working on robustness aspects in ML.

---

<section id="feyza">
<h2>Afra Feyza Akyürek</h2>
</section>

<img src="{{site.baseurl}}/images/feyza_akyurek.jpeg" width="170px" align="bottom">

#### Talk Title

Editing Language Models with Natural Language Feedback

#### Abstract

Even the most sophisticated language models are not immune to inaccuracies, bias or becoming obsolete, highlighting the need for efficient model editing. Model editing involves altering a model's knowledge or representations to achieve specific outcomes without the need for extensive retraining. Traditional research has focused on editing factual data within a narrow scope—limited to knowledge triplets like 'subject-object-relation.' Yet, as language model applications broaden, so does the necessity for diverse editing approaches. In this talk, I will describe our work that introduces a novel dataset where edit requests are natural language sequences, expanding the editing capabilities beyond factual adjustments to encompass a more comprehensive suite of modifications including bias mitigation. This development not only enhances the precision of language models but also increases their adaptability to evolving information and application demands.

#### Speaker Biography

Feyza is a fifth year PhD student in natural language processing advised by Prof. Derry Wijaya. Her research focuses on editing and improving machine learning models after deployment. Examples to post-hoc editing include increasing number of classes that an object classifier recognizes, improving the outputs of language models through machine generated feedback and model editing with natural language instructions.






